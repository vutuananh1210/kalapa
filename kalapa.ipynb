{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kalapa emsembling",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPuZQ43umiSuEAtzEd6mJzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vutuananh1210/kalapa/blob/master/kalapa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1IUVj_dCUmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e1333a3-f51d-408e-a993-67522246c291"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUslsX14CoNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/My Drive/kalapa/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/My Drive/kalapa/test.csv')\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5-PxtRHGg_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22321a10-1b8f-456e-8aba-297b3b24349f"
      },
      "source": [
        "date_time = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\n",
        "date = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 35, 40]]\n",
        "other_date = [\"F_startDate\", \"F_endDate\", \"E_startDate\", \"E_endDate\", \"C_startDate\",\n",
        "              \"C_endDate\", \"G_startDate\", \"G_endDate\", \"A_startDate\", \"A_endDate\"\n",
        "             ]\n",
        "time = date + date_time + other_date\n",
        "\n",
        "def split_date_train(s):\n",
        "    try:\n",
        "        date = s.split('-')\n",
        "        return date\n",
        "    except:\n",
        "        return np.nan\n",
        "def split_date_test(s):\n",
        "    try:\n",
        "        date = s.split('/')\n",
        "        return date\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df_train_c = df_train.copy()\n",
        "df_test_c =  df_test.copy()\n",
        "for col in date:\n",
        "    df_train_c[col] = df_train_c[col].apply(split_date_train)\n",
        "for col in date:\n",
        "    df_test_c[col] = df_test_c[col].apply(split_date_test)\n",
        "\n",
        "new_col = []\n",
        "for col in date: \n",
        "    df_train_c[col+'year'] =[s[0] if type(s) == list else np.nan  for s in df_train_c[col]]\n",
        "    df_train_c[col+'month'] =[s[1]if type(s) == list and len(s) >1  else np.nan for s in df_train_c[col]]\n",
        "    df_train_c[col+'day'] =[s[2]if type(s) == list and len(s) > 2  else np.nan for s in df_train_c[col]]\n",
        "    new_col.append(col+'year')\n",
        "    new_col.append(col+'month')\n",
        "    new_col.append(col+'day')\n",
        "    \n",
        "# the columns in \"date\" are the ones with format year/month/day in train set   \n",
        "\n",
        "#new_col_test = []\n",
        "for col in date: \n",
        "    df_test_c[col+'year'] =[s[2]if type(s) == list and len(s) > 2  else np.nan for s in df_test_c[col]]\n",
        "    df_test_c[col+'month'] =[s[0] if type(s) == list else np.nan  for s in df_test_c[col]]\n",
        "    df_test_c[col+'day'] =[s[1]if type(s) == list and len(s) >1  else np.nan for s in df_test_c[col]]\n",
        "    #new_col_test.append(col+'year')\n",
        "    #new_col_test.append(col+'month')\n",
        "    #new_col_test.append(col+'day')\n",
        "    \n",
        "# the columns in \"date\" are the ones with format month/day/year in test set  \n",
        "\n",
        "\n",
        "df_train_c = df_train_c.drop(date,1)\n",
        "df_test_c = df_test_c.drop(date,1)\n",
        "# and then check if the columns are match between train and test\n",
        "\n",
        "list(df_train_c)[2:] == list(df_test_c)[1:]\n",
        "\n",
        "def get_date(s):\n",
        "    try:\n",
        "        return s[:10]\n",
        "    except:\n",
        "        return np.nan\n",
        "    \n",
        "for col in date_time:\n",
        "    df_train_c[col] = df_train_c[col].apply(get_date)\n",
        "    df_test_c[col] = df_test_c[col].apply(get_date)\n",
        "# the columns in date_time at the format year/month/day in both train/test set\n",
        "for col in date_time:\n",
        "    df_train_c[col+'year']=[s[:4]if type(s) == str else np.nan for s in df_train_c[col]]\n",
        "    \n",
        "    df_train_c[col+'month']=[s[5:7]if type(s) == str else np.nan for s in df_train_c[col]]\n",
        "    \n",
        "    df_train_c[col+'day']=[s[8:10] if type(s) == str else np.nan for s in df_train_c[col]]\n",
        "\n",
        "new_col_1 = []\n",
        "\n",
        "for col in date_time:\n",
        "    df_test_c[col+'year']=[s[:4]if type(s) == str else np.nan for s in df_test_c[col]]\n",
        "    \n",
        "    df_test_c[col+'month']=[s[5:7]if type(s) == str else np.nan for s in df_test_c[col]]\n",
        "    \n",
        "    df_test_c[col+'day']=[s[8:10] if type(s) == str else np.nan for s in df_test_c[col]]\n",
        "    new_col_1.append(col+'year')\n",
        "    new_col_1.append(col+'month')\n",
        "    new_col_1.append(col+'day')\n",
        "\n",
        "\n",
        "df_train_c = df_train_c.drop(date_time,1)\n",
        "\n",
        "df_test_c = df_test_c.drop(date_time,1)\n",
        "\n",
        "#check again if the columns are match\n",
        "\n",
        "list(df_train_c)[2:] == list(df_test_c)[1:]\n",
        "\n",
        "for col in other_date:\n",
        "    df_train_c[col] = df_train_c[col].apply(split_date_train)\n",
        "for col in other_date: \n",
        "    df_train_c[col+'year'] =[s[0] if type(s) == list else np.nan  for s in df_train_c[col]]\n",
        "    df_train_c[col+'month'] =[s[1]if type(s) == list and len(s) >1  else np.nan for s in df_train_c[col]]\n",
        "    df_train_c[col+'day'] =[s[2]if type(s) == list and len(s) > 2  else np.nan for s in df_train_c[col]]\n",
        "    \n",
        "    \n",
        "# this the data in other_date in format year-month-day in train set\n",
        "\n",
        "for col in other_date:\n",
        "    df_test_c[col] = df_test_c[col].apply(split_date_test)\n",
        "new_col_2 = []\n",
        "for col in other_date: \n",
        "    df_test_c[col+'year'] =[s[2]if type(s) == list and len(s) > 2  else np.nan for s in df_test_c[col]]\n",
        "    df_test_c[col+'month'] =[s[0] if type(s) == list else np.nan  for s in df_test_c[col]]\n",
        "    df_test_c[col+'day'] =[s[1]if type(s) == list and len(s) >1  else np.nan for s in df_test_c[col]]\n",
        "    new_col_2.append(col+'year')\n",
        "    new_col_2.append(col+'month')\n",
        "    new_col_2.append(col+'day')\n",
        "    \n",
        "# the columns in \"date\" are the ones with format month/day/year in test set   \n",
        "\n",
        "df_train_c = df_train_c.drop(other_date,1)\n",
        "df_test_c = df_test_c.drop(other_date,1)\n",
        "\n",
        "list(df_train_c)[2:] == list(df_test_c)[1:]\n",
        "\n",
        "#convert to string first\n",
        "df_train_c[\"ngaySinh\"]= df_train_c[\"ngaySinh\"].apply(str)\n",
        "df_test_c[\"ngaySinh\"] = df_test_c[\"ngaySinh\"].apply(str)\n",
        "\n",
        "def get_age(s):\n",
        "    try:\n",
        "        return 2020 - int(s[:4])\n",
        "    except:\n",
        "        return np.nan\n",
        "    \n",
        "df_train_c[\"ngaySinh\"]= df_train_c[\"ngaySinh\"].apply(get_age)\n",
        "df_test_c[\"ngaySinh\"] = df_test_c[\"ngaySinh\"].apply(get_age)\n",
        "        \n",
        "#process \"Field_34\"\n",
        "df_train_c[\"Field_34\"]= df_train_c[\"Field_34\"].apply(str)\n",
        "df_test_c[\"Field_34\"] = df_test_c[\"Field_34\"].apply(str)\n",
        "def get_year_34(s):\n",
        "  try:\n",
        "    return (s[:4])\n",
        "  except:\n",
        "    return np.nan\n",
        "def get_month_34(s):\n",
        "  try:\n",
        "    return (s[4:6])\n",
        "  except:\n",
        "    return np.nan\n",
        "\n",
        "df_train_c[\"Field_34year\"]= df_train_c[\"Field_34\"].apply(get_year_34)\n",
        "df_train_c[\"Field_34month\"] = df_train_c[\"Field_34\"].apply(get_month_34)\n",
        "\n",
        "df_test_c[\"Field_34year\"]= df_test_c[\"Field_34\"].apply(get_year_34)\n",
        "df_test_c[\"Field_34month\"] = df_test_c[\"Field_34\"].apply(get_month_34)\n",
        "\n",
        "df_train_c = df_train_c.drop(\"Field_34\",1)\n",
        "\n",
        "df_test_c = df_test_c.drop(\"Field_34\",1)\n",
        "\n",
        "print(list(df_train_c)[2:] == list(df_test_c)[1:])\n",
        "\n",
        "#the list of province in Vietnam\n",
        "province_list = ['An Giang','Bà Rịa – Vũng Tàu','Bắc Giang','Bắc Kạn','Bạc Liêu','Bắc Ninh','Bến Tre',\n",
        "'Bình Định','Bình Dương','Bình Phước','Bình Thuận','Cà Mau','Cần Thơ','Cao Bằng','Đà Nẵng','Đắk Lắk',\n",
        "'Đắk Nông','Điện Biên','Đồng Nai','Đồng Tháp','Gia Lai','Hà Giang','Hà Nam','Hà Nội','Hà Tĩnh','Hải Dương',\n",
        "'Hải Phòng','Hậu Giang','Hòa Bình','Hưng Yên','Khánh Hòa','Kiên Giang','Kon Tum','Lai Châu','Lâm Đồng',\n",
        "'Lạng Sơn','Lào Cai','Long An','Nam Định','Nghệ An','Ninh Bình','Ninh Thuận','Phú Thọ','Phú Yên',\n",
        "'Quảng Bình','Quảng Nam','Quảng Ngãi','Quảng Ninh','Quảng Trị','Sóc Trăng','Sơn La','Tây Ninh',\n",
        "'Thái Bình','Thái Nguyên','Thanh Hóa','Huế','Tiền Giang','Hồ Chí Minh','Trà Vinh','Tuyên Quang','Vĩnh Long',\n",
        "'Vĩnh Phúc','Yên Bái' ]\n",
        "#make all lower case\n",
        "province_list = [i.lower() for i in province_list]\n",
        "\n",
        "field_46 = pd.read_csv(\"/content/drive/My Drive/kalapa/kalapa preprocessing/kalapa local/field_46_group.csv\")\n",
        "field_46 = field_46.replace(np.nan, '@#', regex=True)\n",
        "for i in list(field_46):\n",
        "  field_46[i] = field_46[i].str.lower()\n",
        "#fucntion to correct diachi\n",
        "def diachi(s):\n",
        "  for i in province_list:\n",
        "    try:\n",
        "      if i in s:\n",
        "        return i\n",
        "    except:\n",
        "      pass\n",
        "  return s\n",
        "\n",
        "# fucntion to correct Field_46\n",
        "\n",
        "def group_46(s):\n",
        "  for col in list(field_46):\n",
        "    for i in set(field_46[col]):\n",
        "      try:\n",
        "        if i in s:\n",
        "          return col \n",
        "      except:\n",
        "        pass \n",
        "  return \"None\"\n",
        "\n",
        "df_fe = df_train_c.append(df_test_c)\n",
        "df_fe['Field_46'] = df_fe['Field_46'].str.lower()\n",
        "df_fe['Field_46'] = df_fe['Field_46'].apply(group_46)\n",
        "\n",
        "\n",
        "#Field_49\n",
        "df_fe['Field_49'] = df_fe['Field_49'].str.lower()\n",
        "df_fe['Field_49'] = df_fe['Field_49'] .apply(diachi)\n",
        "def group_hcm(s):\n",
        "  district_num = [ 'q.1','q.2','q.3','q.4','q.5','q.6','q.7','q.8','q.9'\n",
        "                   ,'q.10','q.11','q.12',\n",
        "                   'q1','q2','q3','q4','q5','q6','q7','q8','q9','q10','q11','q12','q.']\n",
        "  name = ['hcm', 'saigon','sài gòn','quận','tpcm']\n",
        "  district_str = ['tân phú','phú nhuận','thủ đức','gò vấp',\n",
        "                  'bình thạnh','bình tân','củ chi','hóc môn'\n",
        "                  ,'bình chánh','nhà bè','cần giờ','tân bình']\n",
        "  hcm_city = district_num + name + district_str\n",
        "  for i in hcm_city:\n",
        "    try:\n",
        "      if i in s:\n",
        "        return 'hồ chí minh'\n",
        "    except:\n",
        "      pass\n",
        "  return s\n",
        "\n",
        "df_fe['Field_49'] = df_fe['Field_49'].apply(group_hcm)\n",
        "\n",
        "\n",
        "# replace all the rest with nan\n",
        "def replace_more(s):\n",
        "  for i in province_list:\n",
        "    try:\n",
        "      if i in s: \n",
        "        return s         \n",
        "    except:\n",
        "      pass\n",
        "  return \"None\"\n",
        "  \n",
        "\n",
        "df_fe['Field_49'] = df_fe['Field_49'].apply(replace_more)\n",
        "\n",
        "df_fe['diaChi'] = df_fe['diaChi'].str.lower()\n",
        "df_fe['diaChi'] = df_fe['diaChi'].apply(diachi)\n",
        "df_fe['diaChi'] = df_fe['diaChi'].apply(group_hcm)\n",
        "df_fe['diaChi'] = df_fe['diaChi'].apply(replace_more)\n",
        "\n",
        "\n",
        "df_fe['Field_56'] = df_fe['Field_56'].str.lower()\n",
        "from collections import Counter\n",
        "counter_56 = Counter(df_fe['Field_56'])\n",
        "dict_56 = { \"ngoài quốc doanh\": [\"ngoài quốc doanh\"],\n",
        "           \n",
        "            \"nhà nước\":[\"nhà nước\",'đoàn thể',\"lực lượng vũ trang\",\"đại biểu\"],\n",
        "           \n",
        "           \"doanh nghiệp\":[\"doanh nghiệp\"], \n",
        "           \n",
        "           \"ngoài công lập\":[\"ngoài công lập\"],\n",
        "           \n",
        "           \"hành chánh\": [\"hành chánh\",'sự nghiệp quận',\"sự nghiệp\"],\n",
        "           \n",
        "           \"xã\": ['phường','xã', \"hợp tác xã\"],\n",
        "           \n",
        "           \"nghèo\": [\"nghèo\",\"thất nghiệp\",\"trợ cấp\",\"khó khăn\",\"bảo trợ\"], \n",
        "           \n",
        "           \"hộ gia đình\": [\"hộ gia đình\", \"sinh viên\", \"cá thể\",\"thân nhân\",\"đối tượng\"],\n",
        "           \n",
        "           \"others\": [\"nước ngoài\"]\n",
        "           \n",
        "           \n",
        "          }\n",
        "\n",
        "def group_56(s):\n",
        "    for col in list(dict_56):\n",
        "        for i in dict_56[col]:\n",
        "            try:\n",
        "                if i in s:\n",
        "                    return col\n",
        "            except:\n",
        "                pass\n",
        "    return \"None\"\n",
        "\n",
        "df_fe[\"Field_56\"] = df_fe[\"Field_56\"].apply(group_56)\n",
        "\n",
        "\n",
        "dict_61 = { \"ngoài quốc doanh\": [\"ngoài quốc doanh\"], \"hộ gia đình\":[\"hộ gia đình\",\"cán bộ\",\"cá thể\"],  \n",
        "           \n",
        "           \"ngoài công lập\": [\"ngoài công lập\"], \"nghèo\": [\"nghèo\", \"thất nghiệp\", \"dân tộc\",\"khó khăn\",\"trợ cấp\"], \n",
        "           \n",
        "           \"nước ngoài\": [\"dtnn\"],  \"nhà nước\": [\"đại biểu\",\"nhà nước\", \"đảng\"],\n",
        "           \n",
        "           \"others\": [\"sinh viên\",\"thân nhân\",\"người\",\"phường xã\"]\n",
        "           \n",
        "           \n",
        "               \n",
        "\n",
        "          }\n",
        "\n",
        "def group_61(s):\n",
        "    for col in list(dict_61):\n",
        "        for i in dict_61[col]:\n",
        "            try:\n",
        "                if i in s:\n",
        "                    return col\n",
        "            except:\n",
        "                pass\n",
        "    return \"None\"   \n",
        "\n",
        "\n",
        "df_fe['Field_61'] = df_fe['Field_61'].str.lower()\n",
        "df_fe[\"Field_61\"] = df_fe[\"Field_61\"].apply(group_61)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4pHU7zOHRey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "ae194d66-9670-45e9-dc2b-9be0b855b3bd"
      },
      "source": [
        "\n",
        "!pip install unidecode\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "import math\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "import lightgbm as lgb\n",
        "from unidecode import unidecode\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#pd.options.display.max_columns = 50\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def str_normalize(s):\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(' +', \" \", s)\n",
        "    return s\n",
        "\n",
        "def process_location(df):\n",
        "    for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n",
        "                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n",
        "        df[col].replace(0, np.nan, inplace=True)\n",
        "\n",
        "    df[\"currentLocationLocationId\"] = df[\"currentLocationLocationId\"].apply(str_normalize).astype(\"category\")\n",
        "    df[\"homeTownLocationId\"] = df[\"homeTownLocationId\"].apply(str_normalize).astype(\"category\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def job_category(x):\n",
        "    if type(x) == str:\n",
        "        if \"công nhân\" in x or \"cnv\" in x or \"cn\" in x or \"may công nghiệp\" in x or \"lao động\" in x\\\n",
        "        or \"thợ\" in x or \"coõng nhaõn trửùc tieỏp maựy may coõng nghieọp\" in x or \"c.n\" in x or \"lđ\" in x:\n",
        "            return \"CN\"\n",
        "        elif \"giáo viên\" in x or \"gv\" in x or \"gíao viên\" in x:\n",
        "            return \"GV\"\n",
        "        elif \"nhân viên\" in x or \"kế toán\" in x or \"cán bộ\" in x or \"nv\" in x or \"cb\" in x or \"nhõn viờn\" in x:\n",
        "            return \"NV\"\n",
        "        elif \"tài xế\" in x or \"lái\" in x or \"tài xê\" in x:\n",
        "            return \"TX\"\n",
        "        elif \"quản lý\" in x or \"phó phòng\" in x or \"hiệu phó\" in x:\n",
        "            return \"QL\"\n",
        "        elif \"undefined\" in x:\n",
        "            return \"missing\"\n",
        "        elif \"giám đốc\" in x or \"hiệu trưởng\" in x:\n",
        "            return \"GĐ\"\n",
        "        elif \"phục vụ\" in x:\n",
        "            return \"PV\"\n",
        "        elif \"chuyên viên\" in x:\n",
        "            return  \"CV\"\n",
        "        elif \"bác sĩ\" in x or \"dược sĩ\" in x or \"y sĩ\" in x or \"y sỹ\" in x:\n",
        "            return \"BS\"\n",
        "        elif \"y tá\" in x:\n",
        "            return \"YT\"\n",
        "        elif \"hộ sinh\" in x:\n",
        "            return \"HS\"\n",
        "        elif \"chủ tịch\" in x:\n",
        "            return \"CT\"\n",
        "        elif \"bếp\" in x:\n",
        "            return \"ĐB\"\n",
        "        elif \"sư\" in x:\n",
        "            return \"KS\"\n",
        "        elif \"dưỡng\" in x:\n",
        "            return \"ĐD\"\n",
        "        elif \"kỹ thuật\" in x or \"kĩ thuật\" in x:\n",
        "            return \"KTV\"\n",
        "        elif \"diễn viên\" in x:\n",
        "            return \"DV\"\n",
        "        else:\n",
        "            return \"missing\"\n",
        "    else:\n",
        "        return x    \n",
        "    \n",
        "def process_diaChi_maCv(df):\n",
        "    df[\"maCv\"] = df[\"maCv\"].apply(str_normalize).apply(job_category).astype(\"category\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_gender(s):\n",
        "    x, y = s\n",
        "    \n",
        "    if x != x and y != y:\n",
        "        return \"nan\"\n",
        "    \n",
        "    if x != x:\n",
        "        return y.lower()\n",
        "    \n",
        "    return x.lower()\n",
        "\n",
        "def process_gender(df):\n",
        "    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def process_misc(df):        \n",
        "    df[\"subscriberCount\"].replace(0, np.nan, inplace=True)\n",
        "    df[\"friendCount\"].replace(0, np.nan, inplace=True)\n",
        "    \n",
        "    df[\"Field_13\"] = df[\"Field_13\"].apply(lambda x: 1 if x == x else 0)\n",
        "    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n",
        "    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngoài quốc doanh Quận 7\": np.nan})\n",
        "    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n",
        "    \n",
        "    df[\"Field_27\"] = df[\"Field_27\"].replace({0.0: np.nan})\n",
        "    df[\"Field_28\"] = df[\"Field_28\"].replace({0.0: np.nan})\n",
        "        \n",
        "    for col in df.columns:\n",
        "        if df[col].dtype.name == \"object\":\n",
        "            df[col] = df[col].apply(str_normalize).astype(\"category\")\n",
        "            \n",
        "    return df\n",
        "\n",
        "\n",
        "def transform(df):\n",
        "    df = process_gender(df)\n",
        "    df = process_location(df)\n",
        "    df = process_diaChi_maCv(df)\n",
        "    df = process_misc(df)\n",
        "    return df\n",
        "\n",
        "df_new_fe = transform(df_fe)\n",
        "\n",
        "df_new_fe = df_new_fe.drop([\"gioiTinh\",\"info_social_sex\"],1)\n",
        "def convert_int(s):\n",
        "  try:\n",
        "    return int(s)\n",
        "  except:\n",
        "    return np.nan\n",
        "\n",
        "for col in new_col + new_col_1 + new_col_2:\n",
        "  df_new_fe[col] = df_new_fe[col].apply(convert_int)\n",
        "\n",
        "\n",
        "drop_cat = [\"Field_48\", \"Field_45\", \"Field_68\", \"Field_18\", \"Field_55\", \n",
        "            \n",
        "            \"Field_54\", \"Field_36\", \"currentLocationLocationId\", \"homeTownLocationId\", \n",
        "\n",
        "            \"data.basic_info.locale\", \"currentLocationCity\", \"currentLocationCountry\", \n",
        "\n",
        "            \"currentLocationName\", \"currentLocationState\", \"homeTownCity\", \"homeTownName\", \n",
        "\n",
        "            \"homeTownCountry\", \"homeTownState\"\n",
        "\n",
        "\n",
        "\n",
        "            ]\n",
        "\n",
        "df_new_fe = df_new_fe.drop(drop_cat, 1)\n",
        "\n",
        "\n",
        "for i in list(df_new_fe.select_dtypes(['category'])):\n",
        "  print(i + \": num of cat:    \" + str(len(df_new_fe[i].unique())))\n",
        "\n",
        "df_new_fe[\"Field_34month\"] = df_new_fe[\"Field_34month\"].apply(convert_int)\n",
        "\n",
        "df_new_fe[\"Field_34year\"] = df_new_fe[\"Field_34year\"].apply(convert_int)\n",
        "\n",
        "#create one hot encoding for categorical column\n",
        "#def encode_and_bind(original_dataframe, feature_to_encode):\n",
        "    #dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
        "    #res = pd.concat([original_dataframe, dummies], axis=1)\n",
        "    #res = res.drop([feature_to_encode], axis=1)\n",
        "    #return(res)\n",
        "\n",
        "#for col in list(df_new_fe.select_dtypes(['category'])):\n",
        "  #df_new_fe = encode_and_bind(df_new_fe, col)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Field_4: num of cat:    3\n",
            "Field_12: num of cat:    8\n",
            "diaChi: num of cat:    63\n",
            "Field_46: num of cat:    9\n",
            "Field_49: num of cat:    64\n",
            "Field_56: num of cat:    10\n",
            "Field_61: num of cat:    8\n",
            "Field_65: num of cat:    11\n",
            "Field_66: num of cat:    9\n",
            "maCv: num of cat:    18\n",
            "brief: num of cat:    21\n",
            "Field_34year: num of cat:    15\n",
            "Field_34month: num of cat:    13\n",
            "gender: num of cat:    3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls3iNCIQ9Uxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop columns that contains too many categories\n",
        "y_label = df_train[\"label\"]\n",
        "\n",
        "onehot_drop = [\n",
        "               \n",
        "               \"currentLocationLatitude\", \"currentLocationLongitude\", \"homeTownLatitude\",\n",
        "\n",
        "                        \"subscriberCount\", \"homeTownLongitude\"]\n",
        "\n",
        "\n",
        "#missing_col = []\n",
        "#for col in list(df_train):\n",
        "  #if df_train[col].isna().sum()/ 53030 >= 0.75:\n",
        "    #missing_col.append(col)\n",
        "\n",
        "df_new_fe = df_new_fe.drop(onehot_drop,1)\n",
        "#df_new_fe = df_new_fe.drop(missing_col,1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQaHFwZCcNwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a copy to prevent change in original\n",
        "df_new_fe_c = df_new_fe.copy()\n",
        "\n",
        "features_label = [col for col in list(df_new_fe.columns) if col not in list(df_new_fe.select_dtypes(['category'])) + [\"id\", \"label\", \"kfold\"] ]\n",
        "\n",
        "\n",
        "\n",
        "#create one hot encoding for categorical column\n",
        "def encode_and_bind(original_dataframe, feature_to_encode):\n",
        "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
        "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
        "    res = res.drop([feature_to_encode], axis=1)\n",
        "    return(res)\n",
        "\n",
        "\n",
        "\n",
        "for col in list(df_new_fe.select_dtypes(['category'])):\n",
        "  df_new_fe_c = encode_and_bind(df_new_fe_c, col)\n",
        "\n",
        "#for other columns make it category\n",
        "for col in features_label:\n",
        "\n",
        "  df_new_fe_c.loc[:, col] = df_new_fe_c[col].astype(str).astype('category')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZNz8sG4490i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_fe = df_new_fe_c[df_new_fe[\"id\"] < df_train.shape[0]]\n",
        "test_fe = df_new_fe_c[df_new_fe[\"id\"] >= df_train.shape[0]]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NulYWYI0IbFS",
        "colab_type": "text"
      },
      "source": [
        "## Now modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwhZKH_9If5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First thing to is create fold for train_fe\n",
        "#function to create fold in train_fe\n",
        "\n",
        "\n",
        "train_fe[\"kfold\"] = -1\n",
        "# the next step is to randomize the rows of the data\n",
        "train_fe = train_fe.sample(frac=1).reset_index(drop=True)\n",
        "y = df_train.label.values\n",
        "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
        "for f, (t_, v_) in enumerate(kfold.split(X=train_fe, y=y)):\n",
        "  train_fe.loc[v_, 'kfold'] = f\n",
        "\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA0EGP-2TtmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## entity embedding\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics, preprocessing\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "def create_model(data, catcols):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    for c in catcols:   \n",
        "        num_unique_values = int(data[c].nunique())\n",
        "        embed_dim = int(min(np.ceil((num_unique_values)/2), 80))\n",
        "        inp = layers.Input(shape=(1,))\n",
        "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)     \n",
        "        out = layers.SpatialDropout1D(0.3)(out)\n",
        "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
        "        inputs.append(inp)\n",
        "        outputs.append(out)\n",
        "    x = layers.Concatenate()(outputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inputs, outputs=y)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnjOw91VI71D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "0f0a59b9-84ed-4b09-aff5-58814966545b"
      },
      "source": [
        "# we try xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def run_xgb(fold,train_fe, test_fe):\n",
        "  features = [f for f in train_fe.columns if f not in (\"id\",\"label\", \"kfold\")]\n",
        "\n",
        "\n",
        "  train_fold = train_fe[train_fe.kfold != fold].reset_index(drop=True)\n",
        "  valid_fold = train_fe[train_fe.kfold == fold].reset_index(drop=True) \n",
        "\n",
        "\n",
        "\n",
        "  # get training/valid data\n",
        "  x_train = train_fold[features].values\n",
        "  x_valid = valid_fold[features].values\n",
        "\n",
        "  x_test = test_fe[features].values\n",
        "\n",
        "\n",
        "  model = xgb.XGBClassifier(n_jobs=-1, max_depth=7, n_estimators=200)\n",
        "  model.fit(x_train, train_fold.label.values)\n",
        "\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "  auc = metrics.roc_auc_score(valid_fold.label.values, valid_preds)\n",
        "  #gini = 2*auc -1 \n",
        "\n",
        "  # print gini score\n",
        "  print(f\"Fold = {fold}, Gini = {2*auc-1}\")\n",
        "  pred = model.predict_proba(x_test)[:,1]\n",
        "  return pred\n",
        "train_fe_c = train_fe.copy()\n",
        "test_fe_c = test_fe.copy()\n",
        "\n",
        "predict_xgb = np.zeros(len(df_test[\"id\"]))\n",
        "\n",
        "for fold in range(5):\n",
        "\n",
        "  print(\"training in fold :\", fold)\n",
        "  pred = run_xgb(fold, train_fe_c, test_fe_c)\n",
        "\n",
        "  predict_xgb = predict_xgb + pred\n",
        "\n",
        "\n",
        "  print(\"--------------------------\")\n",
        "\n",
        "\n",
        "predict_xgb = predict_xgb/5\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training in fold : 0\n",
            "Fold = 0, Gini = 0.510631901600622\n",
            "--------------------------\n",
            "training in fold : 1\n",
            "Fold = 1, Gini = 0.49584055961084994\n",
            "--------------------------\n",
            "training in fold : 2\n",
            "Fold = 2, Gini = 0.48520644197168306\n",
            "--------------------------\n",
            "training in fold : 3\n",
            "Fold = 3, Gini = 0.5038887645123022\n",
            "--------------------------\n",
            "training in fold : 4\n",
            "Fold = 4, Gini = 0.49503735974682495\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelCA1qVZgWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#with a plain xgboost and without tunning parameter got 0.49474 GINI in public leaderboard \n",
        "\n",
        "\n",
        "df_test[\"label\"] = predict_xgb\n",
        "df_test[['id', 'label']].to_csv('submission_2.csv', index=False)\n"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}